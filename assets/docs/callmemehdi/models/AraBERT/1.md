# Module callmemehdi/arabert/1

AraBERT is an Arabic pretrained lanaguage model based on Google's BERT base architechture.

<!-- asset-path: https://gsoctfarabert.web.app/arabert.tar.gz -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- license: custom -->

## Overview

The original AraBERT base v2 repository is available in this [link](https://huggingface.co/aubmindlab/bert-base-arabertv2)

These results are obtained from pre-training BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language.The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches

## Performance

Performance of AraBERT on Arabic downstreamtasks compared to mBERT
Results:

 Task | metric | mBERT | AraBERT
------|--------|------ | -------
SA (HARD)        | Acc.   | 95.7  | 96.1
SA (ASTD)        | Acc.   | 80.1  | 96.5
SA (ArsenTD-Lev) | Acc.   | 51.0  | 59.4
SA (AJGT)        | Acc.   | 83.6  | 93.8
SA (LABR)        | Acc.   | 83.0  | 86.7
NER (ANERcorp) | macro-F1   | 78.4  | 81.9
QA (ARCD) | macro-F1   | 61.3  | 62.7


## Usage

In order to use this model, you need to use the arabert repository to preprocess your input.
First, clone their repository using this command:
```shell
git clone https://github.com/aub-mind/arabert.git
```
Then, use it to preprocess your input, here's an example:

```python
from arabert.preprocess import ArabertPreprocessor

model_name="bert-base-arabertv2"
arabert_prep = ArabertPreprocessor(model_name=model_name)

text = "كان الملك رجلا والملكة مرأة"
arabert_prep.preprocess(text)
```
Note that you would need to install some external libraries:
```shell
pip install pyarabic
pip install farasapy
```
After preprocessing the input, we can start tokenizing it:

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("aubmindlab/bert-base-arabertv2")

tokenized_text = tokenizer.tokenize(preprocessed_text)

input = tokenizer.convert_tokens_to_ids(tokenized_text)
```
After that, you have your input ready, so you just have to pass it through the model

```python
import tensorflow as tf

outputs = model(tf.convert_to_tensor([input]))

```

We need to take the last hidden state with ``` outputs[0] ``` which has the following shape:

```
(1, number_of_words, 768)

With number_of_words the number of words in the input text
```


## Reference

```
@inproceedings{antoun2020arabert,
  title={AraBERT: Transformer-based Model for Arabic Language Understanding},
  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference 11--16 May 2020},
  pages={9}
}
```

## License

[SOFTWARE LICENSE AGREEMENT - AraBERT](https://github.com/aub-mind/arabert/blob/master/arabert/LICENSE)
