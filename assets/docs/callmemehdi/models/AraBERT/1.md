# Module callmemehdi/arabert/1

AraBERT is an Arabic pretrained lanaguage model based on Google's BERT architechture.

<!-- asset-path: https://gsoctfarabert.web.app/arabert.tar.gz -->
<!-- task: text-embedding -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- license: custom -->

## Overview

These results are obtained from pre-training BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language.The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches

## Performance

Performance of AraBERT on Arabic downstreamtasks compared to mBERT
Results:

 Task | metric | mBERT | AraBERT
------|--------|------ | -------
SA (HARD)        | Acc.   | 95.7  | 96.1
SA (ASTD)        | Acc.   | 80.1  | 96.5
SA (ArsenTD-Lev) | Acc.   | 51.0  | 59.4
SA (AJGT)        | Acc.   | 83.6  | 93.8
SA (LABR)        | Acc.   | 83.0  | 86.7
NER (ANERcorp) | macro-F1   | 78.4  | 81.9
QA (ARCD) | macro-F1   | 61.3  | 62.7


## Reference

```
@inproceedings{antoun2020arabert,
  title={AraBERT: Transformer-based Model for Arabic Language Understanding},
  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  booktitle={LREC 2020 Workshop Language Resources and Evaluation Conference 11--16 May 2020},
  pages={9}
}
```

## License

[SOFTWARE LICENSE AGREEMENT - AraBERT](https://github.com/snakers4/silero-models/blob/master/LICENSE)
